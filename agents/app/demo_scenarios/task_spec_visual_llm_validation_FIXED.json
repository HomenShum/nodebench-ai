{
  "goal": "Visual LLM validation workflow: search for test images, validate and filter them, analyze with GPT-5-mini and Gemini 2.5 Flash using real vision APIs, compare results, and recommend best model",
  "type": "orchestrate",
  "topic": "Visual LLM Model Validation for VR Avatar Quality Assessment (GPT-5-mini vs Gemini 2.5 Flash)",
  "graph": {
    "nodes": [
      {
        "id": "image_search",
        "kind": "search",
        "label": "Search VR Avatar Test Images",
        "prompt": "VR avatars virtual reality characters full body 3D models hands feet eyes clothing",
        "includeImages": true,
        "depth": "standard"
      },
      {
        "id": "image_validation",
        "kind": "custom",
        "tool": "image.validate",
        "label": "Validate Image URLs",
        "payload": "{{channel:image_search.last}}"
      },
      {
        "id": "image_filtering",
        "kind": "custom",
        "tool": "image.filter",
        "label": "Filter Valid Images",
        "payload": {
          "dataset": "{{channel:image_validation.last}}",
          "filters": {
            "validOnly": true,
            "formats": ["jpeg", "jpg", "png"],
            "maxSize": 1048576
          }
        }
      },
      {
        "id": "vision_analysis",
        "kind": "custom",
        "tool": "vision.multi",
        "label": "Parallel Vision Analysis (GPT-5-mini + Gemini 2.5 Flash)",
        "payload": {
          "dataset": "{{channel:image_filtering.last}}",
          "models": ["gpt-5-mini", "gemini-2.0-flash"],
          "analysisPrompt": "Analyze this VR avatar image for quality issues.\n\nDetect:\n1. Visual artifacts (redlines, distortions, glitches)\n2. Movement quality issues (frozen feet, static fingers)\n3. Eye rendering problems (red lines, artifacts)\n4. Clothing distortions\n\nRate on 1-5 scale:\n- movementMotion (1=worst/frozen, 5=best/natural)\n- visualQuality (1=worst/distorted, 5=best/clean)\n- emotionalComfort (1=worst/unsettling, 5=best/comfortable)\n\nProvide confidence score 0-1 and detailed findings."
        }
      },
      {
        "id": "statistical_analysis",
        "kind": "code.exec",
        "label": "Statistical Analysis & Aggregation",
        "prompt": "Using Python with pandas, numpy, and scipy:\n\n1. Parse vision analysis results: {{channel:vision_analysis.last}}\n2. Calculate inter-model agreement using Pearson correlation for each rating metric (movementMotion, visualQuality, emotionalComfort)\n3. Compute mean, median, std dev for each metric per model\n4. Calculate average ratings across both models per image\n5. Identify outliers (ratings >2 std dev from mean)\n6. Create correlation matrix between models\n\nReturn JSON with:\n{\n  \"agreement\": {\"movementMotion\": float, \"visualQuality\": float, \"emotionalComfort\": float},\n  \"averages\": {\"gpt-5-mini\": {...}, \"gemini-2.0-flash\": {...}},\n  \"outliers\": [...],\n  \"correlation_matrix\": [[...]],\n  \"summary\": \"...\"\n}"
      },
      {
        "id": "visualization",
        "kind": "code.exec",
        "label": "Generate Plotly Visualizations",
        "prompt": "Using Python with plotly, create interactive visualizations from: {{channel:statistical_analysis.last}}\n\nGenerate:\n1. Heatmap of model agreement (correlation matrix)\n2. Box plots of rating distributions per model\n3. Bar chart comparing average ratings across models\n4. Scatter plot of confidence vs ratings\n5. Grouped bar chart of artifact detection rates\n\nSave plots as HTML strings and return JSON with:\n{\n  \"plots\": {\"heatmap\": \"<html>...\", \"boxplot\": \"<html>...\", ...},\n  \"summary\": \"Description of key findings from visualizations\"\n}"
      },
      {
        "id": "model_comparison",
        "kind": "structured",
        "label": "Model Performance Comparison",
        "prompt": "Based on statistical analysis {{channel:statistical_analysis.last}} and visualizations {{channel:visualization.last}}, compare GPT-5-mini vs Gemini 2.5 Flash.\n\nIdentify:\n1. Overall best model (highest average ratings, lowest variance, best artifact detection)\n2. Model rankings with scores\n3. Strengths and weaknesses of each model\n4. Task-specific recommendations (which model is best for redline detection, movement assessment, emotional comfort)\n5. When to use each model\n6. Cost-effectiveness analysis\n\nReturn JSON with schema:\n{\n  \"overallBestModel\": string,\n  \"modelRankings\": [{\"modelName\": string, \"overallScore\": number, \"strengths\": string[], \"weaknesses\": string[]}],\n  \"taskSpecificRecommendations\": {\"redlineDetection\": string, \"movementAssessment\": string, \"emotionalComfort\": string},\n  \"usageGuidelines\": string,\n  \"costEffectiveness\": string\n}"
      },
      {
        "id": "prompt_optimization",
        "kind": "answer",
        "label": "Enhanced Prompt Generation",
        "prompt": "Based on model comparison {{channel:model_comparison.last}}, generate enhanced prompts to improve GPT-5-mini and Gemini 2.5 Flash performance.\n\nFor each model, suggest:\n1. Specific prompt improvements to increase artifact detection accuracy (especially redlines and distortions)\n2. Additional context or examples to improve rating consistency\n3. Structured output schema refinements\n4. Model-specific optimizations (GPT-5-mini tends to be more conservative, Gemini more detailed)\n\nProvide 2 enhanced prompt templates (one for each model) with explanations of improvements and expected performance gains."
      },
      {
        "id": "eval_quality",
        "kind": "eval",
        "label": "Quality Check & Follow-up",
        "prompt": "Evaluate the completeness of the analysis. Check if:\n1. Images were found and validated successfully\n2. Vision analysis completed for both models\n3. Statistical analysis completed successfully\n4. Visualizations were generated\n5. Model comparison identified clear winner\n\nIf any step failed or results are unclear, add nodes to re-run that phase with improved prompts. If quality is good, mark as complete."
      }
    ],
    "edges": [
      { "from": "image_search", "to": "image_validation" },
      { "from": "image_validation", "to": "image_filtering" },
      { "from": "image_filtering", "to": "vision_analysis" },
      { "from": "vision_analysis", "to": "statistical_analysis" },
      { "from": "statistical_analysis", "to": "visualization" },
      { "from": "visualization", "to": "model_comparison" },
      { "from": "model_comparison", "to": "prompt_optimization" },
      { "from": "prompt_optimization", "to": "eval_quality" }
    ]
  },
  "constraints": {
    "maxSteps": 20
  },
  "planHints": [
    "use includeImages: true for image search",
    "use image.validate tool for URL validation",
    "use image.filter tool for filtering",
    "use vision.multi tool for parallel vision analysis",
    "use code.exec for statistical analysis and visualization",
    "use structured outputs for model comparison",
    "use eval nodes for quality checks"
  ],
  "notes": {
    "description": "This workflow demonstrates dual-model visual LLM validation using GPT-5-mini and Gemini 2.5 Flash with REAL vision APIs. It searches for VR avatar test images with Linkup, validates and filters them, analyzes them with both vision models in parallel using actual vision APIs, performs statistical analysis and visualization, compares model performance, and generates enhanced prompts for each model.",
    "requiredEnv": [
      "OPENAI_API_KEY - for GPT-5-mini vision analysis",
      "GOOGLE_GENAI_API_KEY - for Gemini 2.5 Flash vision and code execution",
      "LINKUP_API_KEY - for image search (optional, will use fallback images if missing)"
    ],
    "usage": "npx tsx agents/app/cli.ts agents/app/demo_scenarios/task_spec_visual_llm_validation_FIXED.json",
    "expectedDuration": "3-4 minutes",
    "estimatedCost": "$0.40-0.60 per run (8-10 images)",
    "outputArtifacts": [
      "Image search results from Linkup API",
      "Validated image dataset with metadata",
      "Filtered images (valid JPEGs/PNGs under 1MB)",
      "Vision analysis results from GPT-5-mini (real vision API)",
      "Vision analysis results from Gemini 2.5 Flash (real vision API)",
      "Statistical analysis with correlation matrices and outliers",
      "Interactive Plotly visualizations (heatmap, box plots, bar charts, scatter plots)",
      "Model comparison with rankings and task-specific recommendations",
      "Enhanced prompt templates for both models with performance improvement suggestions"
    ],
    "toolsUsed": {
      "web.search": "Linkup API image search with includeImages: true",
      "image.validate": "URL validation with HEAD requests",
      "image.filter": "Filter by validity, format, and size",
      "vision.multi": "Parallel vision analysis with GPT-5-mini and Gemini 2.5 Flash",
      "code.exec": "Google GenAI code execution for statistics and visualization",
      "structured": "Structured output for model comparison",
      "answer": "LLM generation for prompt optimization",
      "eval": "Quality evaluation and dynamic graph extension"
    },
    "modelComparison": {
      "gpt5mini": {
        "strengths": ["Fast inference", "Cost-effective", "Good structured output"],
        "weaknesses": ["May be conservative in ratings", "Less detailed findings"],
        "costPerImage": "$0.024"
      },
      "gemini2.5flash": {
        "strengths": ["Detailed analysis", "Strong artifact detection", "Multimodal reasoning"],
        "weaknesses": ["Slightly slower", "May over-detect artifacts"],
        "costPerImage": "$0.005"
      }
    },
    "fixes": [
      "Added includeImages: true to image_search node",
      "Added image_validation node using image.validate tool",
      "Added image_filtering node using image.filter tool",
      "Replaced structured vision nodes with vision.multi tool for REAL vision API calls",
      "Changed kind: 'custom' to kind: 'code.exec' for statistical analysis and visualization",
      "Updated model name from 'gemini-2.0-flash' to 'gemini-2.5-flash'",
      "Added proper payload structure for custom tool nodes",
      "Updated edges to reflect new node structure"
    ]
  }
}

